# cafe_torchrec/embedding.py

import torch
import torch.nn as nn
import numpy as np
from typing import List, Optional, Dict
import traceback

from torchrec.modules.embedding_configs import EmbeddingBagConfig
from torchrec.modules.embedding_modules import EmbeddingBagCollection
from torchrec.sparse.jagged_tensor import KeyedJaggedTensor, KeyedTensor

try:
    from .hotsketch_manager import HotSketchManager
except ImportError:
    from hotsketch_manager import HotSketchManager


class CafeEmbeddingBagCollection(nn.Module):
    """
    ä½¿ç”¨ TorchRec EmbeddingBagCollection å®ç° CAFE åµŒå…¥é€»è¾‘ã€‚
    æ ¹æ® HotSketch çš„è¿è¡Œæ—¶é‡è¦æ€§åˆ†æ•°ï¼ŒåŠ¨æ€å°†åµŒå…¥æŸ¥æ‰¾è·¯ç”±åˆ°â€œçƒ­â€è¡¨æˆ–â€œå“ˆå¸Œâ€è¡¨ã€‚
    """

    def __init__(
        self,
        embedding_dim: int,
        hot_feature_count: int,
        hash_table_size: int,
        hotsketch_manager: HotSketchManager,
        device: torch.device,
        hot_table_name: str = "hot_embeddings",
        hash_table_name: str = "hash_embeddings",
    ):
        super().__init__()

        if hot_feature_count <= 0 or hash_table_size <= 0:
            raise ValueError("hot_feature_count å’Œ hash_table_size å¿…é¡»ä¸ºæ­£æ•°ã€‚")

        self.embedding_dim = embedding_dim
        # C++ HotSketch å®ç°ä¸­çƒ­ ID ä» 1 å¼€å§‹åˆ†é…ï¼Œå› æ­¤ +1 ç•™å‡ºç©ºé—´
        # (æ³¨æ„: å¦‚æœå·²ç»åœ¨ C++ ä¸­ä¿®å¤ä¸ºä» 0 å¼€å§‹ï¼Œè¿™é‡Œå¯èƒ½ä¸éœ€è¦ +1ï¼Œä½†ä¿ç•™å®ƒé€šå¸¸æ˜¯å®‰å…¨çš„)
        self.hot_feature_count = hot_feature_count + 1
        self.hash_table_size = hash_table_size
        self.manager = hotsketch_manager
        self.hot_table_name = hot_table_name
        self.hash_table_name = hash_table_name

        # å®šä¹‰ç”¨äº KJT å’Œ EBC è·¯ç”±çš„å”¯ä¸€ç‰¹å¾é”®
        self._hot_feature_key = f"{hot_table_name}_feature_key"
        self._hash_feature_key = f"{hash_table_name}_feature_key"

        # é…ç½®çƒ­è¡¨å’Œå“ˆå¸Œè¡¨
        hot_config = EmbeddingBagConfig(
            name=self.hot_table_name,
            embedding_dim=self.embedding_dim,
            num_embeddings=self.hot_feature_count,
            feature_names=[self._hot_feature_key],
        )

        hash_config = EmbeddingBagConfig(
            name=self.hash_table_name,
            embedding_dim=self.embedding_dim,
            num_embeddings=self.hash_table_size,
            feature_names=[self._hash_feature_key],
        )

        self.ebc = EmbeddingBagCollection(
            tables=[hot_config, hash_config],
            device=device,
        )

        # åå‘ä¼ æ’­æ‰€éœ€çŠ¶æ€
        self._input_ids_for_update: Optional[np.ndarray] = None
        self._query_results_for_update: Optional[np.ndarray] = None
        self._needs_grad_processing = False

    def forward(self,
                feature_ids: torch.Tensor,
                offsets: torch.Tensor,
                per_sample_weights: Optional[torch.Tensor] = None
               ) -> torch.Tensor:
        """
        ä½¿ç”¨ HotSketch å’Œ EBC æ‰§è¡ŒåŠ¨æ€åµŒå…¥æŸ¥æ‰¾ã€‚
        """
        if per_sample_weights is not None:
             raise NotImplementedError("CafeEmbeddingBagCollection ä¸æ”¯æŒ per_sample_weightsã€‚")
        
        self._needs_grad_processing = True

        # ç¡®ä¿è¾“å…¥åœ¨ CPU ä¸Šä»¥è¿›è¡Œ HotSketch æŸ¥è¯¢
        feature_ids_cpu = feature_ids.cpu() if feature_ids.device.type != 'cpu' else feature_ids
        offsets_cpu = offsets.cpu() if offsets.device.type != 'cpu' else offsets

        batch_size = len(offsets_cpu)
        num_indices_total = len(feature_ids_cpu)

        if batch_size <= 0 or num_indices_total == 0:
             self._input_ids_for_update = None
             self._query_results_for_update = None
             self._needs_grad_processing = False
             return torch.zeros((batch_size, self.embedding_dim), device=self.ebc.device)

        # 1. æŸ¥è¯¢ HotSketch
        feature_ids_np = feature_ids_cpu.numpy().astype(np.uint32)
        query_results_np = self.manager.query(feature_ids_np)
        query_results = torch.from_numpy(query_results_np).to(torch.int32)

        # å­˜å‚¨çŠ¶æ€ä¾›åå‘ä¼ æ’­ä½¿ç”¨
        self._input_ids_for_update = feature_ids_np
        self._query_results_for_update = query_results_np

        # 2. åˆ†ç¦»çƒ­/å†· ID
        is_hot_mask_cpu = query_results < 0
        is_cold_mask_cpu = ~is_hot_mask_cpu

        hot_indices = torch.abs(query_results[is_hot_mask_cpu]).to(torch.long)
        
        cold_original_ids = query_results[is_cold_mask_cpu]
        cold_indices = (cold_original_ids.to(torch.long) % self.hash_table_size) if cold_original_ids.numel() > 0 else torch.tensor([], dtype=torch.long)

        # åˆå¹¶ä¸º KJT valuesï¼Œå¹¶ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        kjt_values = torch.cat((hot_indices, cold_indices)).to(self.ebc.device, non_blocking=True)

        # 3. è®¡ç®— KJT lengths
        offsets_with_end = torch.cat((offsets_cpu, torch.tensor([num_indices_total], dtype=torch.long)))
        sample_lengths = offsets_with_end[1:] - offsets_with_end[:-1]
        
        sample_indices_map = torch.repeat_interleave(
            torch.arange(batch_size, dtype=torch.long), 
            repeats=sample_lengths.to(torch.long)
        )
        
        hot_lengths = torch.bincount(sample_indices_map, weights=is_hot_mask_cpu.to(torch.int64), minlength=batch_size).to(torch.long)
        cold_lengths = torch.bincount(sample_indices_map, weights=is_cold_mask_cpu.to(torch.int64), minlength=batch_size).to(torch.long)

        # 4. æ„é€  KeyedJaggedTensor
        kjt_lengths = torch.cat((hot_lengths, cold_lengths)).to(self.ebc.device, non_blocking=True)
        kjt = KeyedJaggedTensor(
            keys=[self._hot_feature_key, self._hash_feature_key],
            values=kjt_values,
            lengths=kjt_lengths,
        )

        # 5. EBC å‰å‘ä¼ æ’­
        pooled_embeddings_keyed: KeyedTensor = self.ebc(kjt)

        # 6. åˆå¹¶ç»“æœ
        return pooled_embeddings_keyed[self._hot_feature_key] + pooled_embeddings_keyed[self._hash_feature_key]

    def update_and_migrate(self):
        """
        åå‘ä¼ æ’­åé€»è¾‘ï¼šè·å–æ¢¯åº¦ï¼Œè®¡ç®—é‡è¦æ€§ï¼Œæ›´æ–° Sketchï¼Œå¹¶æ‰§è¡Œæƒé‡è¿ç§»ã€‚
        """
        if not self._needs_grad_processing:
            return

        if self._input_ids_for_update is None or self._query_results_for_update is None:
            # print("  Warning: No forward pass data available stored for update. Skipping.")
            self._needs_grad_processing = False
            return

        scores_np = np.zeros_like(self._input_ids_for_update, dtype=np.float32)
        grad_map: Dict[np.uint32, float] = {}

        try:
            # --- Step 1 & 2: è·å–æ¢¯åº¦å¹¶è®¡ç®—èŒƒæ•° ---
            hot_table_weights = None
            hash_table_weights = None
            found_params = 0

            for name, param in self.ebc.named_parameters():
                is_hot = name == f"embedding_bags.{self.hot_table_name}.weight"
                is_hash = name == f"embedding_bags.{self.hash_table_name}.weight"

                if is_hot: hot_table_weights = param
                if is_hash: hash_table_weights = param

                if (is_hot or is_hash) and param.grad is not None:
                    found_params += 1
                    grad = param.grad

                    # --- [DEBUG] NaN/Inf check commented out for performance ---
                    # if torch.isnan(grad).any() or torch.isinf(grad).any():
                    #      print(f"\nğŸš¨ [CRITICAL ERROR] NaN or Inf found in gradients for table: {name}!")
                    #      raise ValueError("NaN/Inf detected in gradients, stopping before crash.")
                    # -------------------------

                    is_target_mask = (self._query_results_for_update < 0) if is_hot else ~(self._query_results_for_update < 0)
                    original_ids_in_table = self._input_ids_for_update[is_target_mask]
                    query_results_in_table = self._query_results_for_update[is_target_mask]

                    if grad.is_sparse:
                        indices = grad._indices().squeeze().cpu().numpy().astype(np.int64)
                        norms = torch.norm(grad._values().detach(), p=2, dim=1).cpu().numpy()
                        table_idx_to_norm = dict(zip(indices, norms))

                        for i, original_id in enumerate(original_ids_in_table):
                            table_idx = np.int64(abs(query_results_in_table[i])) if is_hot else np.int64(original_id % self.hash_table_size)
                            if table_idx in table_idx_to_norm:
                                grad_map[original_id] = grad_map.get(original_id, 0.0) + table_idx_to_norm[table_idx]

                    elif not grad.is_sparse:
                        grad_dense = grad.detach()
                        num_rows = grad_dense.shape[0]
                        
                        for i, original_id in enumerate(original_ids_in_table):
                            if is_hot:
                                raw_query_val = query_results_in_table[i]
                                table_idx = np.int64(abs(raw_query_val))
                                
                                # --- [DEBUG] Bounds check commented out for performance ---
                                # if table_idx >= num_rows:
                                #     # ... (error printing logic)
                                #     raise ValueError("Hot table index out of bounds detected in Python!")
                                # --------------------

                            else:
                                table_idx = np.int64(original_id % num_rows)
                                
                                # --- [DEBUG] Bounds check commented out for performance ---
                                # if table_idx >= num_rows:
                                #     # ... (error printing logic)
                                #     raise ValueError("Hash table index out of bounds detected!")
                                # --------------------

                            # Basic protection against crash here if logic is wrong, though less likely now
                            if table_idx < num_rows:
                                norm = torch.norm(grad_dense[table_idx], p=2).item()
                                if norm > 0:
                                    grad_map[original_id] = grad_map.get(original_id, 0.0) + norm

            # if found_params < 2:
            #    pass # print("Warning: Could not find both hot and hash weight parameters.")

            for i, original_id in enumerate(self._input_ids_for_update):
                scores_np[i] = grad_map.get(original_id, 0.0)

        except Exception as e:
            print(f"Error during gradient processing: {e}")
            traceback.print_exc()
            self._needs_grad_processing = False
            return

        # --- Step 3: æ›´æ–° HotSketch å¹¶è·å–è¿ç§»æ©ç  ---
        migration_mask_np = self.manager.update(self._input_ids_for_update, scores_np.astype(np.float32))
        migrated_indices = np.where(migration_mask_np == 1)[0]

        # --- Step 4: æ‰§è¡Œè¿ç§» (æƒé‡å¤åˆ¶) ---
        if len(migrated_indices) > 0:
            migrated_original_ids_np = self._input_ids_for_update[migrated_indices]
            
            # å†æ¬¡æŸ¥è¯¢ä»¥è·å–æœ€æ–°çš„çƒ­ç‰¹å¾ ID
            new_query_results_np = self.manager.query(migrated_original_ids_np.astype(np.uint32))
            
            # === [CRITICAL FIX] ä¸¥æ ¼è¿‡æ»¤ ===
            # åªæœ‰å½“ query è¿”å›è´Ÿæ•° (ç¡®å®æ˜¯çƒ­ç‰¹å¾) ä¸”åœ¨æœ‰æ•ˆèŒƒå›´å†…æ—¶ï¼Œæ‰è¿›è¡Œè¿ç§»
            # è¿™ç›´æ¥ä¿®å¤äº†æ—¥å¿—ä¸­è§‚å¯Ÿåˆ°çš„ bug (åŸå§‹ ID è¢«å½“æˆäº†çƒ­ç‰¹å¾ç´¢å¼•)
            valid_hot_mask = (new_query_results_np < 0) & (new_query_results_np >= -self.hot_feature_count)
            
            # if not np.all(valid_hot_mask):
            #      print(f"Warning: Filtered {np.sum(~valid_hot_mask)} invalid migration candidates.")

            # åº”ç”¨è¿‡æ»¤
            final_original_ids_np = migrated_original_ids_np[valid_hot_mask]
            final_hot_indices_np = np.abs(new_query_results_np[valid_hot_mask])
            
            if len(final_original_ids_np) > 0:
                hash_table_size_actual = hash_table_weights.shape[0] # è·å–å“ˆå¸Œè¡¨çš„å®é™…å¤§å°
                source_hash_indices_np = final_original_ids_np % hash_table_size_actual
                target_device = self.ebc.device
                
                migrated_hot_indices = torch.from_numpy(final_hot_indices_np).to(torch.long).to(target_device)
                source_hash_indices = torch.from_numpy(source_hash_indices_np).to(torch.long).to(target_device)

                try:
                    # if hot_table_weights is None or hash_table_weights is None:
                    #      raise AttributeError("Migration failed: Weight parameters not found.")

                    # --- [PERFORMANCE] Removed explicit synchronize calls ---
                    # if self.ebc.device.type == 'cuda':
                    #    torch.cuda.synchronize()
                    # --------------------------------------------

                    with torch.no_grad():
                        source_vectors = hash_table_weights.detach()[source_hash_indices]
                        hot_table_weights[migrated_hot_indices] = source_vectors

                    # --- [PERFORMANCE] Removed explicit synchronize calls ---
                    # if self.ebc.device.type == 'cuda':
                    #    torch.cuda.synchronize()
                    # --------------------------------------------

                except Exception as e:
                    print(f"ERROR during weight copy: {e}")
                    traceback.print_exc()

        self._input_ids_for_update = None
        self._query_results_for_update = None
        self._needs_grad_processing = False

if __name__ == '__main__':
    print("Example usage requires setting up torch.distributed and a valid HotSketchManager.")